{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audio_features = pd.read_pickle('/home/arpitsah/Desktop/Projects Fall-22/DA/domain_adaptation/LSTM-DENSE/speech-emotion-recognition-iemocap/preprocess_info/feature_vectors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(audio_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_dict = {'ang': 0,\n",
    "                'hap': 1,\n",
    "                'exc': 2,\n",
    "                'sad': 3,\n",
    "                'fru': 4,\n",
    "                'fea': 5,\n",
    "                'sur': 6,\n",
    "                'neu': 7,\n",
    "                'xxx': 8,\n",
    "                'oth': 8,\n",
    "                'dis': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['wav_file', 'label', 'mfccs', 'spec_db'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 84)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_features['spec_db'][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=[]\n",
    "for i in range(len(audio_features['mfccs'])):\n",
    "    a=audio_features['spec_db'][i].shape[1]\n",
    "    b.append(a)\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=[]\n",
    "for i in range(len(audio_features['label'])):\n",
    "    a=audio_features['spec_db'][i]\n",
    "    c.append(a)\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsl = []\n",
    "mfccsl = []\n",
    "spec_dbl = []\n",
    "for i in range(len(audio_features['label'])):\n",
    "    if (audio_features['label'][i]) in [0, 1, 3, 7]:\n",
    "        labelsl.append(audio_features['label'][i])\n",
    "        mfccsl.append(audio_features['spec_db'][i])\n",
    "        spec_dbl.append(audio_features['spec_db'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4490"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mfccsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=[]\n",
    "for i in range(len(mfccsl)):\n",
    "    a = mfccsl[i].shape[1]\n",
    "    f.append(a)\n",
    "np.median(np.array(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #num=160\n",
    "# #padd=num-119\n",
    "# #padd=padd/2\n",
    "# #padd=padd+1\n",
    "# #a=np.pad(np.array(mfccsl[0]),[(0,0),(int(padd),int(padd))],mode='constant')\n",
    "# #garbage=a[:,:num]\n",
    "# #garbage.shape\n",
    "\n",
    "# finalmfccs = []\n",
    "# for i in range(len(mfccsl)):\n",
    "#     a = mfccsl[i]\n",
    "#     if a.shape[1]<num:\n",
    "#         padd = num - a.shape[1]\n",
    "#         padd = padd / 2\n",
    "#         padd = padd + 1\n",
    "#         a = np.pad(np.array(a), [(0,0), (int(padd), int(padd))], mode = 'constant')\n",
    "#     garbage = a[:, :num]\n",
    "#     finalmfccs.append(garbage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(finalmfccs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if running 1st time\n",
    "# for i in range(len(labelsl)):\n",
    "#     if labelsl[i] == 3:\n",
    "#         labelsl[i] = 2\n",
    "#     if labelsl[i] == 7:   # Neutral\n",
    "#         labelsl[i] = 3\n",
    "# # 0 is anger, 1 is happiness, 3 is sadness, 7 is neutral\n",
    "# # Updated 0 is anger, 1 is happiness, 2 is sadness and 3 is neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 1708, 0: 1103, 2: 1084, 1: 595})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labelsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4490\n"
     ]
    }
   ],
   "source": [
    "# print(np.array(finalmfccs).shape)     # The Final Pre-processed MFCCS\n",
    "print(len(labelsl))                   # The Final Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(spec_dbl, labelsl, test_size=0.2)\n",
    "\n",
    "## Manually splitting\n",
    "#dataset_split_ratio = int(len(labelsl) * 0.80) \n",
    "#train_finalmfccs = finalmfccs[:dataset_split_ratio]\n",
    "#train_finallabels = labelsl[:dataset_split_ratio]\n",
    "#print(np.array(train_finalmfccs).shape)\n",
    "#\n",
    "#test_finalmfccs = finalmfccs[dataset_split_ratio:]\n",
    "#test_finallabels = labelsl[dataset_split_ratio:]\n",
    "#print(np.array(test_finalmfccs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 225)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1].shape) # (128, 75)\n",
    "print(y_train[5]) # (128, 75) # emotion\n",
    "#(128, 225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 876, 1: 472, 3: 1368, 2: 876})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 340, 0: 227, 2: 208, 1: 123})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "        # Determine the maximum length of the spectrograms in the batch\n",
    "        max_len = max(spec.shape[1] for spec, label in batch)\n",
    "\n",
    "        # Pad the spectrograms with zeros to the maximum length\n",
    "        padded_specs = []\n",
    "        for spec, label in batch:\n",
    "            num_cols = spec.shape[1]\n",
    "            padding = torch.zeros((128, max_len - num_cols))\n",
    "            padded_specs.append(torch.cat([spec, padding], dim=1))\n",
    "\n",
    "        # Concatenate the padded spectrograms into a tensor\n",
    "        specs_tensor = torch.stack(padded_specs, dim=0)\n",
    "\n",
    "        # Convert the labels to PyTorch tensor\n",
    "        labels_tensor = torch.tensor([label for spec, label in batch])\n",
    "        # Create a list of filenames\n",
    "        # filenames_list = [filename for spec, label, filename in batch]\n",
    "\n",
    "        return specs_tensor, labels_tensor#, filenames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, X_train,y_train):\n",
    "        \n",
    "        self.spectrogram = X_train\n",
    "        self.labels = y_train\n",
    "        # self.filename = dataset['wav_file']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectrogram)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.spectrogram[idx]), torch.tensor(self.labels[idx])#, self.filename[idx]\n",
    "\n",
    "\n",
    "train_dataset = Dataset(X_train,y_train)\n",
    "# class AudioDataset_train(Dataset):\n",
    "#     \"\"\"\n",
    "#     IEMOCAP Dataset\n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         self.len = len(X_train)\n",
    "#         self.x_data = torch.from_numpy(np.array(X_train))\n",
    "#         self.y_data = torch.from_numpy(np.array(y_train))\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "    \n",
    "# traindataset = AudioDataset_train()\n",
    "\n",
    "trainloader = DataLoader(dataset = train_dataset,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True,collate_fn=collate_fn)\n",
    "\n",
    "valdataset = Dataset(X_test,y_test)\n",
    "valloader = DataLoader(dataset = valdataset,\n",
    "                        batch_size = batch_size,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(trainloader):\n",
    "    print(batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class AudioDataset_test(Dataset):\n",
    "#     def __init__(self, X_test):\n",
    "        \n",
    "#         self.spectrogram = X_test\n",
    "#         # self.labels = y_train\n",
    "#         # self.filename = dataset['wav_file']\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.spectrogram)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return torch.tensor(self.spectrogram[idx])#, torch.tensor(self.labels[idx])#, self.filename[idx]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testiter = iter(testloader)\n",
    "# features, labels = next(testiter)\n",
    "# features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         # L1 ImgIn shape=(?, 224, 224, 3)\n",
    "#         #    Conv     -> (?, 224, 224, 16)\n",
    "#         #    Pool     -> (?, 112, 112, 16)\n",
    "#         self.layer1 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             torch.nn.Dropout(p=1 - keep_prob))\n",
    "#         # L2 ImgIn shape=(?, 112, 112, 16)\n",
    "#         #    Conv      ->(?, 112, 112, 32)\n",
    "#         #    Pool      ->(?, 56, 56, 32)\n",
    "#         self.layer2 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             torch.nn.Dropout(p=1 - keep_prob))\n",
    "#         # L3 ImgIn shape=(?, 56, 56, 32)\n",
    "#         #    Conv      ->(?, 56, 56, 64)\n",
    "#         #    Pool      ->(?, 28, 28, 64)\n",
    "#         self.layer3 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             torch.nn.Dropout(p=1 - keep_prob))\n",
    "        \n",
    "#         # L4 ImgIn shape=(?, 28, 28, 64)\n",
    "#         #    Conv      ->(?, 28, 28, 16)\n",
    "#         #    Pool      ->(?, 14, 14, 16)\n",
    "#         self.layer4 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(64, 16, kernel_size=1, stride=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             torch.nn.Dropout(p=1 - keep_prob))\n",
    "\n",
    "#         # L4 FC 14x14x16 inputs -> 512 outputs\n",
    "#         self.fc1 = torch.nn.Linear(14 * 14 * 16, 512, bias=True)\n",
    "#         torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "# #         self.layer4 = torch.nn.Sequential(\n",
    "# #             self.fc1,\n",
    "# #             torch.nn.ReLU(),\n",
    "# #             torch.nn.Dropout(p=1 - keep_prob))\n",
    "#         # L5 Final FC 1024 inputs -> 512 outputs\n",
    "        \n",
    "#         self.fc2 = torch.nn.Linear(512, 4, bias=True)\n",
    "#         torch.nn.init.xavier_uniform_(self.fc2.weight) # initialize parameters\n",
    "#         # L6 Final FC 512 inputs -> 4 outputs\n",
    "# #         self.fc3 = torch.nn.Linear(512, 4, bias=True)\n",
    "# #         torch.nn.init.xavier_uniform_(self.fc3.weight) # initialize parameters\n",
    "\n",
    "#         self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.layer1(x)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "# #         print(out.size())\n",
    "#         out = out.view(out.size(0), -1)\n",
    "# #         print(out.size())# Flatten them for FC\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.dropout(out)\n",
    "#         out = self.fc2(out)\n",
    "# #         out = self.fc3(out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DiagonalLinear, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(output_size, input_size))\n",
    "        # Set non-diagonal elements to be constant\n",
    "        with torch.no_grad():\n",
    "            self.weight.fill_diagonal_(1)\n",
    "            for i in range(input_size):\n",
    "                for j in range(output_size):\n",
    "                    if i != j:\n",
    "                        self.weight[j, i] = 0.0\n",
    "                        self.weight[j, i].requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x,self.weight) \n",
    "\n",
    "\n",
    "\n",
    "# Define the scaling network\n",
    "import torch.nn as nn\n",
    "\n",
    "class Crude_Diag(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(Crude_Diag, self).__init__()\n",
    "        \n",
    "        # Set a fixed seed for reproducibility\n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        # Initialize scaling factors and weight matrix\n",
    "        scaling_factors = torch.rand(in_features)\n",
    "        weight = torch.diag(scaling_factors)\n",
    "        \n",
    "        # Modify weight matrix to set non-diagonal elements to zero\n",
    "        with torch.no_grad():\n",
    "            for i in range(in_features):\n",
    "                for j in range(in_features):\n",
    "                    if i != j:\n",
    "                        weight[i, j] = 0.0\n",
    "                        weight[i, j].requires_grad = False\n",
    "        \n",
    "        # Define linear layer with diagonal weight matrix\n",
    "        self.linear = nn.Linear(in_features, in_features, bias=False)\n",
    "        self.linear.weight = nn.Parameter(weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # L1 ImgIn shape=(?, 224, 224, 3)\n",
    "        #    Conv     -> (?, 224, 224, 16)\n",
    "        #    Pool     -> (?, 112, 112, 16)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L2 ImgIn shape=(?, 112, 112, 16)\n",
    "        #    Conv      ->(?, 112, 112, 32)\n",
    "        #    Pool      ->(?, 56, 56, 32)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L3 ImgIn shape=(?, 56, 56, 32)\n",
    "        #    Conv      ->(?, 56, 56, 64)\n",
    "        #    Pool      ->(?, 28, 28, 64)\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        \n",
    "        # L4 ImgIn shape=(?, 28, 28, 64)\n",
    "        #    Conv      ->(?, 28, 28, 16)\n",
    "        #    Pool      ->(?, 14, 14, 16)\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 16, kernel_size=1, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "\n",
    "        # L4 FC 14x14x16 inputs -> 512 outputs\n",
    "        self.fc1 = torch.nn.Linear(14 * 14 * 16, 512, bias=True)\n",
    "        torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "#         self.layer4 = torch.nn.Sequential(\n",
    "#             self.fc1,\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L5 Final FC 1024 inputs -> 512 outputs\n",
    "        \n",
    "        # Affine Layer\n",
    "        self.Diag_Affine = Crude_Diag(512)\n",
    "        self.test_linear = torch.nn.Linear(512,512)\n",
    "        self.fc2 = torch.nn.Linear(512, 4, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight) # initialize parameters\n",
    "        # L6 Final FC 512 inputs -> 4 outputs\n",
    "#         self.fc3 = torch.nn.Linear(512, 4, bias=True)\n",
    "#         torch.nn.init.xavier_uniform_(self.fc3.weight) # initialize parameters\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "#         print(out.size())\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         print(out.size())# Flatten them for FC\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.Diag_Affine(out)\n",
    "        # out = self.test_linear(out)\n",
    "        out = self.fc2(out)\n",
    "#         out = self.fc3(out)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4963, 1.5364, 0.2654, 0.5281]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the scaling network\n",
    "scale_net = Crude_Diag(4)\n",
    "# scale_net = DiagonalLinear(4,4)\n",
    "\n",
    "# Scale some input data\n",
    "input = torch.tensor([[1.0, 2.0, 3.0,4.0]])\n",
    "scaled_input = scale_net(input)\n",
    "# scaled_input = scale_net(input)\n",
    "\n",
    "# Print the scaled input\n",
    "print(scaled_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network before turning off sparse layer 2157332\n",
      "network after turning off sparse layer 1895188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (Diag_Affine): Crude_Diag(\n",
       "    (linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (test_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)     \n",
    "\n",
    "print(\"network before turning off sparse layer\", count_parameters(model))\n",
    "\n",
    "for param in model.Diag_Affine.parameters():\n",
    "    param.requires_grad = False     \n",
    "print(\"network after turning off sparse layer\", count_parameters(model))\n",
    "model = CNN()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "  (Diag_Affine): Crude_Diag(\n",
      "    (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "  )\n",
      "  (test_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger = 0\n",
    "happiness = 0\n",
    "sadness = 0\n",
    "neutral = 0\n",
    "\n",
    "for _, target in trainloader:\n",
    "    labels = list(target.numpy())\n",
    "    anger += len([x for x in labels if x == 0])\n",
    "    happiness += len([x for x in labels if x == 1])\n",
    "    sadness += len([x for x in labels if x == 2])\n",
    "    neutral += len([x for x in labels if x == 3])\n",
    "    \n",
    "for _, target in valloader:\n",
    "    labels = list(target.numpy())\n",
    "    anger += len([x for x in labels if x == 0])\n",
    "    happiness += len([x for x in labels if x == 1])\n",
    "    sadness += len([x for x in labels if x == 2])\n",
    "    neutral += len([x for x in labels if x == 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger 1103 happiness 595 sadness 1084 neutral 1708\n",
      "tensor([0.0009, 0.0017, 0.0009, 0.0006], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "label_to_class = {0: 'anger', 1: 'happiness', 2: 'sadness', 3: 'neutral'}\n",
    "final_labels = ['anger', 'happiness', 'sadness', 'neutral']\n",
    "\n",
    "sample_weights = [1/anger, 1/happiness, 1/sadness, 1/neutral]\n",
    "print(\"anger {} happiness {} sadness {} neutral {}\".format(anger, happiness, sadness, neutral))\n",
    "\n",
    "class_weights = torch.FloatTensor(sample_weights).cuda()\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# device ='cuda'\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f'{total_params:,} total parameters.')\n",
    "# total_trainable_params = sum(\n",
    "#     p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f'{total_trainable_params:,} training parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual, predicted, labels):\n",
    "    cm = confusion_matrix(actual, predicted, labels)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, annot_kws={\"size\": 10}, fmt='.0f'); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # instantiation\n",
    "device='cuda'\n",
    "\n",
    "# model = CNN().to(device)\n",
    "# print(\"network after turning off sparse layer\", count_parameters(net))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model):\n",
    "    model.train()\n",
    "    correct_train = 0\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        print(data.shape)\n",
    "        \n",
    "        # zero the gradient, forward, backward and running pytorch rhythm\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        # get the label of prediction\n",
    "        pred = torch.max(output.data, 1)[1]\n",
    "        correct_train += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\n'.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))\n",
    "    \n",
    "    train_loss /= len(trainloader.dataset)\n",
    "    train_acc = 100. * correct_train / len(trainloader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}\\n'.format(train_loss))\n",
    "    print('\\nTrain Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct_train, len(trainloader.dataset), 100. * correct_train / len(trainloader.dataset)))\n",
    "    \n",
    "    return train_loss, int(train_acc.numpy())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    correct = 0\n",
    "    history_test = []\n",
    "\n",
    "    pred_model = []\n",
    "    actual = []\n",
    "\n",
    "    for data, target in valloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # output from model\n",
    "        output = model(data)\n",
    "\n",
    "        # sum total loss\n",
    "        test_loss += criterion(output, target).item()\n",
    "\n",
    "        # get the label of prediction\n",
    "        pred = torch.max(output.data, 1)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        pred_model.append(pred.cpu().numpy())\n",
    "        actual.append(target.data.cpu().numpy())\n",
    "\n",
    "\n",
    "    test_loss /= len(valloader.dataset)\n",
    "    test_acc = 100. * correct / len(valloader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valloader.dataset),\n",
    "        100. * correct / len(valloader.dataset)))\n",
    "\n",
    "\n",
    "    pred_with_label = [label_to_class[label] for label in list(np.concatenate(pred_model))]\n",
    "    actual_with_label = [label_to_class[label] for label in list(np.concatenate(actual))]\n",
    "\n",
    "    confusion_matrix(actual_with_label, pred_with_label, labels=final_labels)\n",
    "\n",
    "    print('\\n Classification Report \\n {} \\n'.format(classification_report(actual_with_label, pred_with_label)))\n",
    "\n",
    "    return test_loss, int(test_acc.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 128, 956])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m history, n_epoch \u001b[39m=\u001b[39m [], \u001b[39m101\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epoch):    \n\u001b[1;32m      4\u001b[0m     \u001b[39m# exp_lr_scheduler.step(epoch)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m# import pdb\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[39m# pdb.set_trace()\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(epoch, model)\n\u001b[1;32m      8\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test(model)\n\u001b[1;32m     10\u001b[0m     \u001b[39m# plateau_scheduler.step(test_loss)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[139], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# zero the gradient, forward, backward and running pytorch rhythm\u001b[39;00m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     14\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m     15\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/domain_adaption38/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[128], line 116\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m         out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[39m#         print(out.size())# Flatten them for FC\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(out)\n\u001b[1;32m    117\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(out)\n\u001b[1;32m    118\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDiag_Affine(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/domain_adaption38/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/domain_adaption38/lib/python3.8/site-packages/torch/nn/modules/linear.py:93\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/domain_adaption38/lib/python3.8/site-packages/torch/nn/functional.py:1690\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, tens_ops, \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[1;32m   1688\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m bias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1689\u001b[0m     \u001b[39m# fused op is marginally faster\u001b[39;00m\n\u001b[0;32m-> 1690\u001b[0m     ret \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(bias, \u001b[39minput\u001b[39;49m, weight\u001b[39m.\u001b[39;49mt())\n\u001b[1;32m   1691\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1692\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mmatmul(weight\u001b[39m.\u001b[39mt())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "history, n_epoch = [], 101\n",
    "\n",
    "for epoch in range(1, n_epoch):    \n",
    "    # exp_lr_scheduler.step(epoch)\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    train_loss, train_acc = train(epoch, model)\n",
    "    test_loss, test_acc = test(model)\n",
    "    \n",
    "    # plateau_scheduler.step(test_loss)\n",
    "    history.append([train_loss, train_acc, test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history, columns=[\"train_loss\", \"train_acc\", \"test_loss\", \"test_acc\"])\n",
    "history_df[\"epoch\"] = [x for x in range(1, n_epoch)]\n",
    "print(history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.lineplot(x='epoch', y='train_loss', data=history_df, color='b')\n",
    "# plt.xticks(history_df.epoch)\n",
    "sns.lineplot(x='epoch', y='test_loss', data=history_df, color='g')\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.lineplot(x='epoch', y='train_acc', data=history_df, color='b')\n",
    "# plt.xticks(history_df.epoch)\n",
    "sns.lineplot(x='epoch', y='test_acc', data=history_df, color='g')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
