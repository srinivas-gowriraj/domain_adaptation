{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audio_features = pd.read_pickle('/home/arpitsah/Desktop/Projects Fall-22/DA/domain_adaptation/LSTM-DENSE/speech-emotion-recognition-iemocap/preprocess_info/feature_vectors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(audio_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_dict = {'ang': 0,\n",
    "                'hap': 1,\n",
    "                'exc': 2,\n",
    "                'sad': 3,\n",
    "                'fru': 4,\n",
    "                'fea': 5,\n",
    "                'sur': 6,\n",
    "                'neu': 7,\n",
    "                'xxx': 8,\n",
    "                'oth': 8,\n",
    "                'dis': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['wav_file', 'label', 'mfccs', 'spec_db'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 84)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_features['spec_db'][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=[]\n",
    "for i in range(len(audio_features['mfccs'])):\n",
    "    a=audio_features['spec_db'][i].shape[1]\n",
    "    b.append(a)\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=[]\n",
    "for i in range(len(audio_features['label'])):\n",
    "    a=audio_features['spec_db'][i]\n",
    "    c.append(a)\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsl = []\n",
    "mfccsl = []\n",
    "spec_dbl = []\n",
    "for i in range(len(audio_features['label'])):\n",
    "    if (audio_features['label'][i]) in [0, 1, 3, 7]:\n",
    "        labelsl.append(audio_features['label'][i])\n",
    "        mfccsl.append(audio_features['spec_db'][i])\n",
    "        spec_dbl.append(audio_features['spec_db'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4490"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mfccsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=[]\n",
    "for i in range(len(mfccsl)):\n",
    "    a = mfccsl[i].shape[1]\n",
    "    f.append(a)\n",
    "np.median(np.array(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #num=160\n",
    "# #padd=num-119\n",
    "# #padd=padd/2\n",
    "# #padd=padd+1\n",
    "# #a=np.pad(np.array(mfccsl[0]),[(0,0),(int(padd),int(padd))],mode='constant')\n",
    "# #garbage=a[:,:num]\n",
    "# #garbage.shape\n",
    "\n",
    "# finalmfccs = []\n",
    "# for i in range(len(mfccsl)):\n",
    "#     a = mfccsl[i]\n",
    "#     if a.shape[1]<num:\n",
    "#         padd = num - a.shape[1]\n",
    "#         padd = padd / 2\n",
    "#         padd = padd + 1\n",
    "#         a = np.pad(np.array(a), [(0,0), (int(padd), int(padd))], mode = 'constant')\n",
    "#     garbage = a[:, :num]\n",
    "#     finalmfccs.append(garbage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(finalmfccs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if running 1st time\n",
    "# for i in range(len(labelsl)):\n",
    "#     if labelsl[i] == 3:\n",
    "#         labelsl[i] = 2\n",
    "#     if labelsl[i] == 7:   # Neutral\n",
    "#         labelsl[i] = 3\n",
    "# # 0 is anger, 1 is happiness, 3 is sadness, 7 is neutral\n",
    "# # Updated 0 is anger, 1 is happiness, 2 is sadness and 3 is neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({7: 1708, 0: 1103, 3: 1084, 1: 595})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labelsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4490\n"
     ]
    }
   ],
   "source": [
    "# print(np.array(finalmfccs).shape)     # The Final Pre-processed MFCCS\n",
    "print(len(labelsl))                   # The Final Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(spec_dbl, labelsl, test_size=0.2)\n",
    "\n",
    "## Manually splitting\n",
    "#dataset_split_ratio = int(len(labelsl) * 0.80) \n",
    "#train_finalmfccs = finalmfccs[:dataset_split_ratio]\n",
    "#train_finallabels = labelsl[:dataset_split_ratio]\n",
    "#print(np.array(train_finalmfccs).shape)\n",
    "#\n",
    "#test_finalmfccs = finalmfccs[dataset_split_ratio:]\n",
    "#test_finallabels = labelsl[dataset_split_ratio:]\n",
    "#print(np.array(test_finalmfccs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 100)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1].shape) # (128, 75)\n",
    "print(y_train[5]) # (128, 75) # emotion\n",
    "#(128, 225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 874, 7: 1376, 3: 867, 1: 475})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 229, 7: 332, 1: 120, 3: 217})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "        # Determine the maximum length of the spectrograms in the batch\n",
    "        max_len = max(spec.shape[1] for spec, label in batch)\n",
    "\n",
    "        # Pad the spectrograms with zeros to the maximum length\n",
    "        padded_specs = []\n",
    "        for spec, label in batch:\n",
    "            num_cols = spec.shape[1]\n",
    "            padding = torch.zeros((128, max_len - num_cols))\n",
    "            padded_specs.append(torch.cat([spec, padding], dim=1))\n",
    "\n",
    "        # Concatenate the padded spectrograms into a tensor\n",
    "        specs_tensor = torch.stack(padded_specs, dim=0)\n",
    "\n",
    "        # Convert the labels to PyTorch tensor\n",
    "        labels_tensor = torch.tensor([label for spec, label in batch])\n",
    "        # Create a list of filenames\n",
    "        # filenames_list = [filename for spec, label, filename in batch]\n",
    "\n",
    "        return specs_tensor, labels_tensor#, filenames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, X_train,y_train):\n",
    "        \n",
    "        self.spectrogram = X_train\n",
    "        self.labels = y_train\n",
    "        # self.filename = dataset['wav_file']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectrogram)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.spectrogram[idx]), torch.tensor(self.labels[idx])#, self.filename[idx]\n",
    "\n",
    "\n",
    "train_dataset = Dataset(X_train,y_train)\n",
    "# class AudioDataset_train(Dataset):\n",
    "#     \"\"\"\n",
    "#     IEMOCAP Dataset\n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         self.len = len(X_train)\n",
    "#         self.x_data = torch.from_numpy(np.array(X_train))\n",
    "#         self.y_data = torch.from_numpy(np.array(y_train))\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "    \n",
    "# traindataset = AudioDataset_train()\n",
    "\n",
    "trainloader = DataLoader(dataset = train_dataset,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True,collate_fn=collate_fn)\n",
    "\n",
    "valdataset = Dataset(X_test,y_test)\n",
    "valloader = DataLoader(dataset = valdataset,\n",
    "                        batch_size = batch_size,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(trainloader):\n",
    "    print(batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class AudioDataset_test(Dataset):\n",
    "#     def __init__(self, X_test):\n",
    "        \n",
    "#         self.spectrogram = X_test\n",
    "#         # self.labels = y_train\n",
    "#         # self.filename = dataset['wav_file']\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.spectrogram)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return torch.tensor(self.spectrogram[idx])#, torch.tensor(self.labels[idx])#, self.filename[idx]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testiter = iter(testloader)\n",
    "# features, labels = next(testiter)\n",
    "# features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         # L1 ImgIn shape=(?, 224, 224, 3)\n",
    "#         #    Conv     -> (?, 224, 224, 16)\n",
    "#         #    Pool     -> (?, 112, 112, 16)\n",
    "#         self.layer1 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             torch.nn.Dropout(p=1 - keep_prob))\n",
    "#         # L2 ImgIn shape=(?, 112, 112, 16)\n",
    "#         #    Conv      ->(?, 112, 112, 32)\n",
    "#         #    Pool      ->(?, 56, 56, 32)\n",
    "#         self.layer2 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             torch.nn.Dropout(p=1 - keep_prob))\n",
    "#         # L3 ImgIn shape=(?, 56, 56, 32)\n",
    "#         #    Conv      ->(?, 56, 56, 64)\n",
    "#         #    Pool      ->(?, 28, 28, 64)\n",
    "#         self.layer3 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             torch.nn.Dropout(p=1 - keep_prob))\n",
    "        \n",
    "#         # L4 ImgIn shape=(?, 28, 28, 64)\n",
    "#         #    Conv      ->(?, 28, 28, 16)\n",
    "#         #    Pool      ->(?, 14, 14, 16)\n",
    "#         self.layer4 = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(64, 16, kernel_size=1, stride=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             torch.nn.Dropout(p=1 - keep_prob))\n",
    "\n",
    "#         # L4 FC 14x14x16 inputs -> 512 outputs\n",
    "#         self.fc1 = torch.nn.Linear(14 * 14 * 16, 512, bias=True)\n",
    "#         torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "# #         self.layer4 = torch.nn.Sequential(\n",
    "# #             self.fc1,\n",
    "# #             torch.nn.ReLU(),\n",
    "# #             torch.nn.Dropout(p=1 - keep_prob))\n",
    "#         # L5 Final FC 1024 inputs -> 512 outputs\n",
    "        \n",
    "#         self.fc2 = torch.nn.Linear(512, 4, bias=True)\n",
    "#         torch.nn.init.xavier_uniform_(self.fc2.weight) # initialize parameters\n",
    "#         # L6 Final FC 512 inputs -> 4 outputs\n",
    "# #         self.fc3 = torch.nn.Linear(512, 4, bias=True)\n",
    "# #         torch.nn.init.xavier_uniform_(self.fc3.weight) # initialize parameters\n",
    "\n",
    "#         self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.layer1(x)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "# #         print(out.size())\n",
    "#         out = out.view(out.size(0), -1)\n",
    "# #         print(out.size())# Flatten them for FC\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.dropout(out)\n",
    "#         out = self.fc2(out)\n",
    "# #         out = self.fc3(out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob =0.5\n",
    "class Crude_Diag(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(Crude_Diag, self).__init__()\n",
    "        \n",
    "        # Set a fixed seed for reproducibility\n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        # Initialize scaling factors and weight matrix\n",
    "        scaling_factors = torch.rand(in_features)\n",
    "        weight = torch.diag(scaling_factors)\n",
    "        \n",
    "        # Modify weight matrix to set non-diagonal elements to zero\n",
    "        with torch.no_grad():\n",
    "            for i in range(in_features):\n",
    "                for j in range(in_features):\n",
    "                    if i != j:\n",
    "                        weight[i, j] = 0.0\n",
    "                        weight[i, j].requires_grad = False\n",
    "        \n",
    "        # Define linear layer with diagonal weight matrix\n",
    "        self.linear = nn.Linear(in_features, in_features, bias=False)\n",
    "        self.linear.weight = nn.Parameter(weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # L1 ImgIn shape=(?, 224, 224, 3)\n",
    "        #    Conv     -> (?, 224, 224, 16)\n",
    "        #    Pool     -> (?, 112, 112, 16)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L2 ImgIn shape=(?, 112, 112, 16)\n",
    "        #    Conv      ->(?, 112, 112, 32)\n",
    "        #    Pool      ->(?, 56, 56, 32)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L3 ImgIn shape=(?, 56, 56, 32)\n",
    "        #    Conv      ->(?, 56, 56, 64)\n",
    "        #    Pool      ->(?, 28, 28, 64)\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        \n",
    "        # L4 ImgIn shape=(?, 28, 28, 64)\n",
    "        #    Conv      ->(?, 28, 28, 16)\n",
    "        #    Pool      ->(?, 14, 14, 16)\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 16, kernel_size=1, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "\n",
    "        # L4 FC 14x14x16 inputs -> 512 outputs\n",
    "        self.fc1 = torch.nn.Linear(512, 512, bias=True)\n",
    "        torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "#         self.layer4 = torch.nn.Sequential(\n",
    "#             self.fc1,\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L5 Final FC 1024 inputs -> 512 outputs\n",
    "        self.pool =  torch.nn.AdaptiveAvgPool2d((1,1))\n",
    "        # Affine Layer\n",
    "        self.Diag_Affine = Crude_Diag(512)\n",
    "        # self.test_linear = torch.nn.Linear(512,512)\n",
    "        self.fc2 = torch.nn.Linear(512, 4, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight) # initialize parameters\n",
    "        # L6 Final FC 512 inputs -> 4 outputs\n",
    "#         self.fc3 = torch.nn.Linear(512, 4, bias=True)\n",
    "#         torch.nn.init.xavier_uniform_(self.fc3.weight) # initialize parameters\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # out = self.layer4(out)\n",
    "#         print(out.size())\n",
    "        # out = out.view(out.size(0), -1)\n",
    "        # out = out.mean(axis = -1)\n",
    "        out = torch.mean(out, axis=-1).reshape(-1,512)\n",
    "#         print(out.size())# Flatten them for FC\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.Diag_Affine(out)\n",
    "        # out = self.test_linear(out)\n",
    "        out = self.fc2(out)\n",
    "#         out = self.fc3(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0328, -0.2532, -0.0147, -0.5407],\n",
      "        [ 0.4180, -0.3902, -0.8919, -0.1810],\n",
      "        [-1.0130, -0.0062, -0.0935,  0.1081],\n",
      "        [ 0.1796,  0.1419, -0.6890, -0.5396],\n",
      "        [-1.0630,  0.1698, -0.4283, -0.7475],\n",
      "        [-0.5486, -0.1372, -0.0305, -0.6501],\n",
      "        [-0.0423, -0.3204, -0.3573, -0.7357],\n",
      "        [ 0.2947, -0.1449, -0.2917, -0.4626]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "    model = CNN()\n",
    "    input = torch.randn(64,13,33)\n",
    "    out = model(input)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)     \n",
    "\n",
    "print(\"network before turning off sparse layer\", count_parameters(model))\n",
    "\n",
    "for param in model.Diag_Affine.parameters():\n",
    "    param.requires_grad = False     \n",
    "print(\"network after turning off sparse layer\", count_parameters(model))\n",
    "model = CNN()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger = 0\n",
    "happiness = 0\n",
    "sadness = 0\n",
    "neutral = 0\n",
    "\n",
    "for _, target in trainloader:\n",
    "    labels = list(target.numpy())\n",
    "    anger += len([x for x in labels if x == 0])\n",
    "    happiness += len([x for x in labels if x == 1])\n",
    "    sadness += len([x for x in labels if x == 2])\n",
    "    neutral += len([x for x in labels if x == 3])\n",
    "    \n",
    "for _, target in valloader:\n",
    "    labels = list(target.numpy())\n",
    "    anger += len([x for x in labels if x == 0])\n",
    "    happiness += len([x for x in labels if x == 1])\n",
    "    sadness += len([x for x in labels if x == 2])\n",
    "    neutral += len([x for x in labels if x == 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_class = {0: 'anger', 1: 'happiness', 2: 'sadness', 3: 'neutral'}\n",
    "final_labels = ['anger', 'happiness', 'sadness', 'neutral']\n",
    "\n",
    "sample_weights = [1/anger, 1/happiness, 1/sadness, 1/neutral]\n",
    "print(\"anger {} happiness {} sadness {} neutral {}\".format(anger, happiness, sadness, neutral))\n",
    "\n",
    "class_weights = torch.FloatTensor(sample_weights).cuda()\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# device ='cuda'\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f'{total_params:,} total parameters.')\n",
    "# total_trainable_params = sum(\n",
    "#     p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f'{total_trainable_params:,} training parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual, predicted, labels):\n",
    "    cm = confusion_matrix(actual, predicted, labels)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, annot_kws={\"size\": 10}, fmt='.0f'); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # instantiation\n",
    "device='cuda'\n",
    "\n",
    "# model = CNN().to(device)\n",
    "# print(\"network after turning off sparse layer\", count_parameters(net))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model):\n",
    "    model.train()\n",
    "    correct_train = 0\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        print(data.shape)\n",
    "        \n",
    "        # zero the gradient, forward, backward and running pytorch rhythm\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        # get the label of prediction\n",
    "        pred = torch.max(output.data, 1)[1]\n",
    "        correct_train += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\n'.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))\n",
    "    \n",
    "    train_loss /= len(trainloader.dataset)\n",
    "    train_acc = 100. * correct_train / len(trainloader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}\\n'.format(train_loss))\n",
    "    print('\\nTrain Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct_train, len(trainloader.dataset), 100. * correct_train / len(trainloader.dataset)))\n",
    "    \n",
    "    return train_loss, int(train_acc.numpy())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    correct = 0\n",
    "    history_test = []\n",
    "\n",
    "    pred_model = []\n",
    "    actual = []\n",
    "\n",
    "    for data, target in valloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # output from model\n",
    "        output = model(data)\n",
    "\n",
    "        # sum total loss\n",
    "        test_loss += criterion(output, target).item()\n",
    "\n",
    "        # get the label of prediction\n",
    "        pred = torch.max(output.data, 1)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        pred_model.append(pred.cpu().numpy())\n",
    "        actual.append(target.data.cpu().numpy())\n",
    "\n",
    "\n",
    "    test_loss /= len(valloader.dataset)\n",
    "    test_acc = 100. * correct / len(valloader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valloader.dataset),\n",
    "        100. * correct / len(valloader.dataset)))\n",
    "\n",
    "\n",
    "    pred_with_label = [label_to_class[label] for label in list(np.concatenate(pred_model))]\n",
    "    actual_with_label = [label_to_class[label] for label in list(np.concatenate(actual))]\n",
    "\n",
    "    confusion_matrix(actual_with_label, pred_with_label, labels=final_labels)\n",
    "\n",
    "    print('\\n Classification Report \\n {} \\n'.format(classification_report(actual_with_label, pred_with_label)))\n",
    "\n",
    "    return test_loss, int(test_acc.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history, n_epoch = [], 101\n",
    "\n",
    "for epoch in range(1, n_epoch):    \n",
    "    # exp_lr_scheduler.step(epoch)\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    train_loss, train_acc = train(epoch, model)\n",
    "    test_loss, test_acc = test(model)\n",
    "    \n",
    "    # plateau_scheduler.step(test_loss)\n",
    "    history.append([train_loss, train_acc, test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history, columns=[\"train_loss\", \"train_acc\", \"test_loss\", \"test_acc\"])\n",
    "history_df[\"epoch\"] = [x for x in range(1, n_epoch)]\n",
    "print(history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.lineplot(x='epoch', y='train_loss', data=history_df, color='b')\n",
    "# plt.xticks(history_df.epoch)\n",
    "sns.lineplot(x='epoch', y='test_loss', data=history_df, color='g')\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.lineplot(x='epoch', y='train_acc', data=history_df, color='b')\n",
    "# plt.xticks(history_df.epoch)\n",
    "sns.lineplot(x='epoch', y='test_acc', data=history_df, color='g')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
